{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iPEJTLyTjE7dViyguzUIhJWIL2Qagvv1",
      "authorship_tag": "ABX9TyMvNWaudae/WFRCgPYH1Wat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RTrombini/Projeto_Aluno_Especial_Unicamp/blob/main/Projeto_unicamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Projeto Information Retrieval**\n",
        "\n",
        " \n",
        "\n",
        "O objetivo final desse projeto é fornecer um programa em que o usuário digite oque busca e receba de volta os documentos da coleção CISI que melhor se encaixam.\n",
        "\n",
        " \n",
        "\n",
        "Os requisitos foram:\n",
        "\n",
        " \n",
        "\n",
        "- Utilizar o algoritmo BM25\n",
        "\n",
        "- Utilizar o diretorio CISI para testes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KZ2GtaJY-4YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metodologia**\n",
        "\n",
        " \n",
        "\n",
        "Consultando o chatGPT sobre como seria a maneira mais eficiente de fazer um sistema de Information Retrival, obtivemos a seguinte sugestão de passos:\n",
        "\n",
        " \n",
        "\n",
        "*1- Preparar os dados: Para criar um sistema de recuperação de informações, você precisará de um conjunto de dados contendo os documentos que deseja indexar. Esses documentos podem ser textos de sites da web, artigos, livros ou qualquer outra coisa que você queira indexar. Para este exemplo, vamos usar um conjunto de dados de artigos sobre tecnologia.*\n",
        "\n",
        " \n",
        "\n",
        "*2- Indexar os documentos: Para indexar os documentos, você pode usar uma biblioteca de indexação, como o Whoosh ou o ElasticSearch. Essas bibliotecas são capazes de extrair informações dos documentos, como os termos que aparecem nos documentos e a frequência com que aparecem. Eles também permitem que você armazene essas informações em um formato que permita consultas rápidas.*\n",
        "\n",
        " \n",
        "\n",
        "*3- Implementar o algoritmo BM25: Para implementar o algoritmo BM25, você pode usar a biblioteca python-whoosh, que fornece uma implementação do BM25. Você precisará calcular o BM25 para cada termo da consulta em relação a cada documento indexado. A biblioteca python-whoosh permite que você calcule o BM25 com base nos termos da consulta e nos termos indexados nos documentos.*\n",
        "\n",
        " \n",
        "\n",
        "*5- Retornar os resultados: Depois de calcular o BM25 para cada documento e , você precisará retornar os resultados ao usuário. Os resultados podem ser classificados com base na pontuação BM25 ou em qualquer outra métrica que você preferir. Você pode retornar uma lista dos documentos mais relevantes ou exibir um snippet do texto em cada documento.*\n",
        "\n",
        " \n",
        "\n",
        "Além de perguntar quais passos seriam recomendados para realizar a tarefa, perguntamos quais bibliotecas facilitariam o processo:\n",
        "\n",
        " \n",
        "\n",
        "*1- Whoosh: É uma biblioteca de indexação em Python que permite indexar e pesquisar documentos em Python. É fácil de usar e oferece uma implementação do algoritmo BM25 para classificar documentos.*\n",
        "\n",
        " \n",
        "\n",
        "*2- NLTK (Natural Language Toolkit): É uma biblioteca de processamento de linguagem natural em Python que fornece recursos para pré-processamento de texto, tokenização, análise sintática e outras tarefas relacionadas à linguagem natural.*\n",
        "\n",
        " \n",
        "\n",
        "*3- Pandas: É uma biblioteca de análise de dados em Python que permite manipular e analisar dados em Python. É útil para carregar e manipular dados de documentos e conjuntos de dados.*\n",
        "\n",
        " \n",
        "\n",
        "*4- Flask: É um framework web em Python que permite construir aplicativos web com facilidade. É útil para criar uma interface do usuário para o sistema de recuperação de informações.*\n",
        "\n",
        " \n",
        "\n",
        "Como o projeto deve ser implementado no Google Colab, não há a necessidade de cosntruir um aplicativo com Flask e como já temos um dataset pre-definido (CISI), não iremos precisar utilizar o Pandas para manipulação dos dados.\n",
        "\n",
        " \n",
        "\n",
        "Não consegui implementar um algoritmo com o NLTK para a ferramenta de buscas no colab pois não possuo experiencia previa com essa biblioteca nem com o colab e não teria tempo o bastante para fazer algo significativo com ela.\n",
        "\n",
        " \n",
        "\n",
        "Apesar das demais sugestões do chatGPT, utilizaei apenas a priemeira lib sugerida (Whoosh) "
      ],
      "metadata": {
        "id": "2AkSqGnr_DY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalação das biblitecas e arquivos necessários**\n"
      ],
      "metadata": {
        "id": "t6bftYgvRjSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install whoosh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGvidWxGh_mm",
        "outputId": "4ac2fd47-8c46-4f33-9bff-8b0acd935666"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro precisamos fazer o download do arquivo CISI indicado para os testes. "
      ],
      "metadata": {
        "id": "KAQX_tA6PdlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "url = 'http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz'\n",
        "r = requests.get(url, allow_redirects=True)"
      ],
      "metadata": {
        "id": "lwugtKtHPcpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "\n",
        "my_tar = tarfile.open('cisi.tar.gz')\n",
        "my_tar.extractall('/content/drive/MyDrive/Colab_Notebooks') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "metadata": {
        "id": "ZtK5y6O_Real"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo o arquivo CISI ja descompactado no nosso diretorio, o proximo passo é a indexação de todo o conteudo do arquivo cisi.all."
      ],
      "metadata": {
        "id": "_vwsnAw3Rx4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Indexação**\n",
        "\n",
        " \n",
        "\n",
        "Tendo o arquivo CISI ja descompactado no nosso diretorio, o proximo passo é a indexação de todo o conteudo do arquivo cisi.all.\n",
        "\n",
        " \n",
        "\n",
        "Para isso, o importamos as bibliotecas necessárias (os, re, whoosh), definimos o esquema dos campos do índice e cria um indexador para a pasta do índice.\n",
        "\n",
        " \n",
        "\n",
        "Depois, o lemos o arquivo de texto da coleção CISI e usamos expressões regulares para separar os documentos e extrair os campos relevantes (ID, título, autor e resumo). Em seguida, adiciona cada documento ao índice usando o método \"add_document()\" do objeto \"writer\".\n",
        "\n",
        " \n",
        "\n",
        "Por fim, o código finaliza a indexação com o método \"commit()\" do objeto \"writer\".\n",
        "\n",
        " \n",
        "\n",
        "O resultado final é um índice que pode ser usado para pesquisar informações em documentos da coleção CISI.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "gRkWS9Vy_UPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from whoosh import index\n",
        "from whoosh.index import create_in\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "\n",
        "\n",
        "def index_cisi_collection():\n",
        "\n",
        "    if index.exists_in(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\"):\n",
        "        try:\n",
        "            ix = index.open_dir(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\")\n",
        "            writer = ix.writer()\n",
        "        except index.LockError:\n",
        "            ix = index.open_dir(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\", readonly=True)\n",
        "            ix._unlock()\n",
        "            os.remove(os.path.join(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\", \"write.lock\"))\n",
        "            ix = index.open_dir(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\")\n",
        "            writer = ix.writer()\n",
        "    else:\n",
        "        # Define o schema para os campos do índice\n",
        "        schema = Schema(id=ID(unique=True, stored=True),\n",
        "                        title=TEXT(stored=True),\n",
        "                        author=TEXT(stored=True),\n",
        "                        abstract=TEXT(stored=True))\n",
        "\n",
        "        # Cria o diretório do índice (se ele não existir)\n",
        "        if not os.path.exists(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\"):\n",
        "            os.mkdir(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\")\n",
        "\n",
        "        # Cria um indexador para a pasta do índice\n",
        "        ix = create_in(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\", schema)\n",
        "        writer = ix.writer()\n",
        "\n",
        "    # Abre o arquivo de texto da coleção CISI\n",
        "    with open(\"/content/drive/MyDrive/Colab_Notebooks/cisi/CISI.ALL\", \"r\") as f:\n",
        "        # Lê o arquivo inteiro de uma vez\n",
        "        text = f.read()\n",
        "\n",
        "        # Usa expressões regulares para separar os documentos e extrair os campos relevantes\n",
        "        pattern = r\"\\.I (\\d+)\\n\\.T\\n(.*?)\\n\\.A\\n(.*?)\\n\\.W\\n(.*?)\\n(?:\\.X\\n(.*?))?(?=\\n\\.I|\\Z)\"\n",
        "        docs = re.findall(pattern, text, re.DOTALL)\n",
        "\n",
        "        for doc in docs:\n",
        "            doc_id, title, author, abstract, content, = doc\n",
        "\n",
        "            #Adiciona o documento ao índice\n",
        "            writer.add_document(id=doc_id,\n",
        "                                title=title,\n",
        "                                author=author,\n",
        "                                abstract=abstract)\n",
        "\n",
        "    # Finaliza a indexação\n",
        "    writer.commit()\n"
      ],
      "metadata": {
        "id": "mnYDKi-KR7Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input do Usuário**\n",
        "\n",
        " \n",
        "\n",
        "Este código define uma função chamada highlights() que pede ao usuário para digitar uma frase e retorna as palavras mais importantes dessa frase, ignorando palavras comuns em inglês (como artigos e preposições) e pontuações. O ingles foi escolhido pois os arquivos da pasta cisi estão em inglês.\n",
        "\n",
        " \n",
        "\n",
        "A biblioteca string é importada para utilizar a variável punctuation, que é uma string contendo todos os caracteres de pontuação.\n",
        "\n",
        " \n",
        "\n",
        "O código começa pedindo ao usuário para inserir uma frase e, em seguida, inicializa uma lista vazia chamada results que será preenchida com as cinco palavras mais importantes. Em seguida, é criada uma lista de stop words, que contém palavras comuns em inglês que serão ignoradas. O dicionário word_counts é inicializado para contar a frequência das palavras e as palavras da entrada do usuário são convertidas para minúsculas e divididas em uma lista de palavras.\n",
        "\n",
        " \n",
        "\n",
        "O código percorre cada palavra da lista, ignorando as stop words e pontuações e, em seguida, conta sua frequência no dicionário word_counts. Depois que todas as palavras são contadas, a lista é ordenada em ordem decrescente de frequência e as cinco palavras mais importantes são adicionadas à lista results.\n",
        "\n",
        " \n",
        "\n",
        "Finalmente, a função retorna uma lista com as palavras-chave da entrada fornecida pelo usuário.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "T2fUie0g_WQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def highligths():\n",
        "    # Obter a entrada do usuário\n",
        "    user_input = input(\"Digite uma frase: \")\n",
        "    results = []\n",
        "\n",
        "    # Definir uma lista de palavras comuns em ingles a serem ignoradas\n",
        "    stop_words = ['a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'with']\n",
        "\n",
        "    # Inicializar um dicionário vazio para contar a frequência das palavras\n",
        "    word_counts = {}\n",
        "\n",
        "    # Dividir a entrada do usuário em palavras e converter para letras minúsculas\n",
        "    words = user_input.lower().split()\n",
        "\n",
        "    # Percorrer todas as palavras e contar sua frequência, ignorando as stop words e pontuações\n",
        "    for word in words:\n",
        "        if word not in stop_words and word not in string.punctuation:\n",
        "            if word in word_counts:\n",
        "                word_counts[word] += 1\n",
        "            else:\n",
        "                word_counts[word] = 1\n",
        "\n",
        "    # Ordenar as palavras por frequência\n",
        "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Criar uma lista com as palavras mais importantes\n",
        "    for word, count in sorted_words:\n",
        "        results.append(word)\n",
        "\n",
        "    # Retorna a lista com as palavras mais importantes\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "4ahk28MuSHp4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Busca**\n",
        "\n",
        " \n",
        "\n",
        "Esse codigo utiliza, novamente, a biblioteca whoosh para fazer a busca nos documentos ja indexados. Especificamente, ele importa a função open_dir do módulo index, e várias classes e funções do módulo scoring, qparser, e query.\n",
        "\n",
        " \n",
        "\n",
        "O código define uma função parser que realiza uma busca em um índice de documentos de texto previamente criado, que está localizado na pasta cisi_index. A função recebe uma lista de strings de busca (search_strings) e retorna os resultados da busca.\n",
        "\n",
        " \n",
        "\n",
        "A função usa um objeto MultifieldParser para definir um esquema de busca com três campos - \"abstract\", \"author\" e \"id\" - e um objeto BM25F para definir o algoritmo de pontuação. Em seguida, ela itera sobre as strings de busca fornecidas, criando uma query para cada uma usando o método parse do objeto parser, e as une usando o operador Or. Por fim, a função executa a busca usando o método search do objeto searcher, iterando sobre os resultados e imprimindo as informações relevantes de cada documento.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "ZoXUXcbW_chn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import open_dir\n",
        "from whoosh import scoring, qparser\n",
        "from whoosh.query import Or\n",
        "\n",
        "\n",
        "def parser(search_strings):\n",
        "    # Abre o índice de busca salvo na pasta 'cisi_index'\n",
        "    ix = open_dir(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\")\n",
        "\n",
        "    # Define o esquema de consulta com três campos: abstract, author e id\n",
        "    schema = ix.schema\n",
        "    parser = qparser.MultifieldParser([\"abstract\", \"author\", \"id\"], schema=schema)\n",
        "\n",
        "    # Define o algoritmo BM25 como o algoritmo de pontuação\n",
        "    searcher = ix.searcher(weighting=scoring.BM25F())\n",
        "\n",
        "    # Cria uma lista de queries a partir das strings de busca fornecidas\n",
        "    queries = []\n",
        "    for search_string in search_strings:\n",
        "        queries.append(parser.parse(search_string))\n",
        "\n",
        "    # Une as queries em uma única query usando o operador OR\n",
        "    query = Or(queries)\n",
        "\n",
        "    # Executa a consulta\n",
        "    results = searcher.search(query, limit=None)\n",
        "\n",
        "    # Itera sobre os resultados e imprime as informações relevantes de cada documento\n",
        "    for hit in results:\n",
        "        print(f\"ID: {hit['id']}\")\n",
        "        print(f\"Title: {hit['title']}\")\n",
        "        print(f\"Author: {hit['author']}\")\n",
        "        print(f\"Abstract: {hit['abstract']}\")\n",
        "        print(f\"Score: {hit.score}\")\n",
        "        print(\"\")\n",
        "        print(\"__________________________//____________________\")\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "r_8anjWobsgz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rodando o Script**\n",
        "\n",
        "Ao rodar a célula abaixo, o usuário irá ser indagado a digitar uma entrada para o sistema. \n",
        "\n",
        "O código checa se já existe um diretorio com o arquivo CISI indexado e, caso contrário, faz a indexação com a função *index_cisi_collection()*.\n",
        "\n",
        "Após isso, a função highligths() recebe a entrada do usuário e estima quais as palavras mais relevantes dessa entrada. O resultado dessa função é uma lista que será a entrada da função que realizará a busca em si, utilizando o algoritimo BM25, *parser()*."
      ],
      "metadata": {
        "id": "Oy0l1bciB-L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"/content/drive/MyDrive/Colab_Notebooks/cisi_index\"):\n",
        "    index_cisi_collection()\n",
        "\n",
        "query = highligths()\n",
        "\n",
        "parser(query)"
      ],
      "metadata": {
        "id": "zyL4jtYLSXWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uTCrBXuvINKr"
      }
    }
  ]
}